{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58969b4c-6b3a-44f0-a846-5d90f12c51c5",
   "metadata": {},
   "source": [
    "# Segmentation: example from torchvision\n",
    "\n",
    "We have seen in previous chapters the basics of PyTorch and how to train a simple network. As this course is rather meant to help you *use* networks rather than *train* them, we won't be repeating the training process for other DL modalities. Rather, we will use pre-trained network and see how we should format inputs, how to use outputs etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94a43d-a6f7-4045-9d7e-c59458f4f0a4",
   "metadata": {},
   "source": [
    "## Semantic segmentation\n",
    "\n",
    "Until now we have only seen classification i.e. an image gets summarized to a single number corresponding to a category. The other main class of tasks done in Computer Vision is segmentation i.e. detection of objects in images. There are different types of segmentation: single class segmentation where only one type of object is detected, semantic segmentation where all pixels belonging to objects of a given type are assigned a given class, instance segmentation where each object gets assigned a given class etc.\n",
    "\n",
    "Here we first look at semantic segmentation using a classical convolutional network called ResNet. This network can be found pre-trained in the torchivision library trained on standard datasets. We copied the example below from that library [here](https://pytorch.org/vision/stable/models.html#semantic-segmentation). Let's run it part by part.\n",
    "\n",
    "### Image and model import\n",
    "\n",
    "First we of course need to load an image and the chosen model. Models are imported first from the library as models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a51507e-9990-4eb9-9636-d2bfa0d0df60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "img = read_image(\"../data/woody_baille.JPG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b0d32-6f9e-4f2f-8787-e2c5ac6fa242",
   "metadata": {},
   "source": [
    "We see that we import both a model ```fcn_resnet50``` and a weights object ```FCN_ResNet50_Weights```. The model defines the architecture, i.e. the sequence of layers, while the weigths are the values of all the parameters in the model that have been pre-trained on a given dataset. We can now initialize the weights with default values and load those weights into the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732eaffe-6553-4260-944e-9d15ea274e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): FCNHead(\n",
       "    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux_classifier): FCNHead(\n",
       "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Initialize model with the best available weights\n",
    "weights = FCN_ResNet50_Weights.DEFAULT\n",
    "model = fcn_resnet50(weights=weights)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e8ba9-268a-4721-a6e0-5564fcf40bf5",
   "metadata": {},
   "source": [
    "The last command prints out the model. We see that it is composed of many parts. Importantly, we can check the first and last layer. In the first layer we see that the model expects three channels: this is a model trained on natural images with RGB channels. Then the last layer: ```Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))```. So the last layer is a convolutional layer with 21 outputs. Unlike in classifiers where a list of numbers (probabilities of classes is output) here the output will be an image composed of 21 channels. Essentially, segmentation is classification task for each single pixel. At the end we get 21 images containing the probabilities for each pixel to belong to one of the 21 categories. Why 21? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502d896e-d4dc-4bb3-8279-5f07e782f46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FCN_ResNet50_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec93600-88a1-4835-9ac4-e2bda80bd54d",
   "metadata": {},
   "source": [
    "We see that the default weights correspond to the VOC training, a standard Computer Vision challenge using 20 categories of objects (plus background):\n",
    "- Person: person\n",
    "- Animal: bird, cat, cow, dog, horse, sheep\n",
    "- Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train\n",
    "- Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60df0783-6f12-4f19-bcd1-cab24cd4e7bb",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "Then we need to make sure that we are going to feed the network the appropriate data. First we need to normalize our images with the same weights used during training. For this we load a transform that will take care of this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98a9fd9b-b617-4949-8f18-4e3d155f628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize the inference transforms\n",
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9722f-854b-462c-8c6e-b9f49852c071",
   "metadata": {},
   "source": [
    "Then we need to apply the transform to our image. Note that our images are RGB images, so their dimensions are C x H y W where the two last dimensions are just pixel dimensions. With three dimensions, the model will assume that we actually have a **a batch of size 3 for gray scale images**. So we need to add a dummy dimension in the first position to create a batch of size 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86813a3-04bd-4f27-8286-2c0e2fa5887b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gw18g940/mambaforge/envs/dslpytorch/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Apply inference preprocessing transforms\n",
    "batch = preprocess(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f45a4a4-fc6c-4b92-894a-cb586fbacc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 520, 693])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c9caf7-2999-4ef8-8665-f3b56d07868c",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Finally we can run inference with our model. This particular model as multiple outputs and we just need the ```out``` element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21bd9727-7462-498c-9d32-96d5727858e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(batch)['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caa9520f-a3ab-42d1-9b6e-dbd23d2c1674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 520, 693])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056b035-3be7-4552-b9b0-bf5bbbc4dc6c",
   "metadata": {},
   "source": [
    "So now the size of our output is indeed an image with 21 channels. What we want is a single-channel image where the value of each pixel indicates its class. As the pixel values are not normalized, we compute the softmax value for each \"column of 21 pixels\" to get actual probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "665399fc-e8b6-4136-b2a5-0025e4d33658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Use the model and visualize the prediction\n",
    "normalized_masks = prediction.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bcb9171-4b89-48af-a66c-09e44681bcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 520, 693])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89edbd5-a739-4a8c-acf8-1c91bf759376",
   "metadata": {},
   "source": [
    "Now we can check for each pixel which channel has the largest values, i.e. largest probability and use that channel index as value in our image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9387b55-04ee-4856-9fce-1fe9a9913481",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_class = normalized_masks.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "087afe07-eada-4ded-abb9-08d7453e4a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f13cae78-1375-4176-81b7-3a38918cb4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 520, 693])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "867c0fa1-bda1-4093-80b9-e758accd5b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(pixel_class[0].detach().numpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca67c9e3-32b6-470d-98e6-86a0d9c0086c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  8, 12])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecf1f393-415b-48f0-9198-4ce9bde15563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(303566), tensor(56793), tensor(1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "[torch.sum(pixel_class == x) for x in pixel_class.unique()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7a79d9-e82c-4fc9-aa15-cb1bce0f2f53",
   "metadata": {},
   "source": [
    "If we check our pixel classes, we see that we have only three types of pixels and that after value 0 (background) the majority has index 8. We can have a look at the weights metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ecf2f34-331e-479e-bbe9-51a75f4fe525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__background__',\n",
       " 'aeroplane',\n",
       " 'bicycle',\n",
       " 'bird',\n",
       " 'boat',\n",
       " 'bottle',\n",
       " 'bus',\n",
       " 'car',\n",
       " 'cat',\n",
       " 'chair',\n",
       " 'cow',\n",
       " 'diningtable',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'motorbike',\n",
       " 'person',\n",
       " 'pottedplant',\n",
       " 'sheep',\n",
       " 'sofa',\n",
       " 'train',\n",
       " 'tvmonitor']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81528ff4-e869-4b8b-863e-f031ea355e2a",
   "metadata": {},
   "source": [
    "and see that index 8 indeed corresponds to cats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "619c6262-e507-4f85-b310-475a785b2cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.meta[\"categories\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435652a-3131-4431-9a9e-62ab8b8079fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
